```table-of-contents
```
# Code ban Ä‘áº§u
```python
from urllib.request import Request, urlopen
from urllib.parse import urlparse, urljoin
from urllib.error import URLError
import ssl
from bs4 import BeautifulSoup
import threading
import queue

class Crawler:
    base_url = ''
    myssl = ssl.create_default_context()
    myssl.check_hostname=False
    myssl.verify_mode=ssl.CERT_NONE
    errorLinks = set()
    crawled_links = set()

    def __init__(self, base_url):
        Crawler.base_url = base_url

    @staticmethod
    def crawl(thread_name, url, links_to_crawl):
        try:
            link = urljoin(Crawler.base_url, url)
            if (urlparse(link).netloc == 'tutorialedge.net') and (link not in Crawler.crawled_links):
                request = Request(link, headers={'User-Agent': 'Mozilla/5.0'})
                response = urlopen(request, context=Crawler.myssl)
                Crawler.crawled_links.add(link)
                print("Url {} Crawled with Status: {} : {} Crawled In Total".format(response.geturl(), response.getcode(), len(Crawler.crawled_links)))
                soup = BeautifulSoup(response.read(), "html.parser")
                Crawler.enqueueLinks(soup.find_all('a'), links_to_crawl)
        except URLError as e:
            print("URL {} threw this error when trying to parse: {}".format(link, e.reason))
            Crawler.errorLinks.add(link)

    @staticmethod
    def enqueueLinks(links, links_to_crawl):
        for link in links:
            if (urljoin(Crawler.base_url, link.get('href')) not in Crawler.crawled_links):
                if (urljoin(Crawler.base_url, link.get('href')) not in links_to_crawl):
                    links_to_crawl.put(link.get('href'))


class CheckableQueue(queue.Queue):
    def __contains__(self, item):
        with self.mutex:
            return item in self.queue
    def __len__(self):
        return len(self.queue)


THREAD_COUNT = 100
linksToCrawl = CheckableQueue()

def createCrawlers():
    for i in range(THREAD_COUNT):
        t = threading.Thread(target=run)
        t.daemon = True
        t.start()

def run():
    while True:
        url = linksToCrawl.get()
        try:
            if url is None:
                break
            Crawler.crawl(threading.current_thread(), url, linksToCrawl)
        except:
            print("Exception")
        linksToCrawl.task_done()


def main():
    url = input("Website > ")
    linksToCrawl.put(url)
    createCrawlers()
    linksToCrawl.join()
    print("Total Links Crawled: {}".format(len(Crawler.crawled_links)))
    print("Total Errors: {}".format(len(Crawler.errorLinks)))

if __name__ == '__main__':
    main()

```

## Giáº£i thÃ­ch cÃ¢u há»i má»™t chÃºt

### 1. Vá» cÃ¢u há»i **"queue khÃ´ng há»— trá»£ `in` mÃ  sao láº¡i `item in self.queue`"**?
- ÄÃºng rá»“i, báº£n thÃ¢n `queue.Queue` **khÃ´ng há»— trá»£ trá»±c tiáº¿p** `item in myqueue` trÃªn Ä‘á»‘i tÆ°á»£ng queue bÃªn ngoÃ i.
```python
new_queue = queue.Queue()
# KhÃ´ng há»— trá»£
print("a" in new_queue)
```
- NhÆ°ng **bÃªn trong** `queue.Queue` cÃ³ má»™t **biáº¿n ná»™i bá»™** tÃªn lÃ  `.queue`:
    - `.queue` thÆ°á»ng lÃ  má»™t `collections.deque` hoáº·c `list`.
    - MÃ  `list` vÃ  `deque` thÃ¬ **há»— trá»£** toÃ¡n tá»­ `in`.

âœ… VÃ¬ váº­y, **`item in self.queue`** thá»±c ra lÃ  Ä‘ang check bÃªn trong cÃ¡i **list**/**deque** lÆ°u trá»¯ dá»¯ liá»‡u thá»±c sá»± trong queue.

### 2. Tháº¿ cÃ²n `with self.mutex:` lÃ m gÃ¬?

- `queue.Queue` thiáº¿t káº¿ cho **Ä‘a luá»“ng** (multi-thread).
- VÃ¬ váº­y **má»i thao tÃ¡c truy cáº­p** `.queue` **pháº£i cÃ³ khÃ³a (`mutex`)** Ä‘á»ƒ **trÃ¡nh race condition** (2 thread cÃ¹ng Ä‘á»c/ghi má»™t lÃºc â†’ há»ng dá»¯ liá»‡u).
- `self.mutex` lÃ  má»™t `threading.Lock()`, giÃºp **khÃ³a** queue táº¡m thá»i Ä‘á»ƒ thao tÃ¡c an toÃ n.

ğŸ’¬ Tá»©c lÃ :
```python
with self.mutex:
	return item in self.queue`
```

NghÄ©a lÃ :

> KhÃ³a queue láº¡i, kiá»ƒm tra an toÃ n xem item cÃ³ trong queue ná»™i bá»™ khÃ´ng, rá»“i má»Ÿ khÃ³a.

Náº¿u **khÃ´ng `with self.mutex`**, 2 thread cÃ¹ng lÃºc Ä‘á»c queue thÃ¬ dá»¯ liá»‡u cÃ³ thá»ƒ **khÃ´ng chÃ­nh xÃ¡c**, tháº­m chÃ­ bá»‹ lá»—i crash.

---
# Cáº£i tiáº¿n láº§n 1

CÃ¡c cáº£i tiáº¿n:
* Chá»‰ khá»Ÿi táº¡o má»™t thread pool láº§n Ä‘áº§u, vÃ  cÃ³ nhiá»u thread trong pool
* LÆ°u káº¿t quáº£ vÃ o trong CSV

```python
from urllib.request import Request, urlopen
from urllib.parse import urlparse, urljoin
from urllib.error import URLError
from concurrent.futures import ThreadPoolExecutor, as_completed
import ssl
from bs4 import BeautifulSoup
import threading
import csv
import queue

class Crawler:
    base_url = ''
    myssl = ssl.create_default_context()
    myssl.check_hostname=False
    myssl.verify_mode=ssl.CERT_NONE
    errorLinks = set()
    crawled_links = set()

    def __init__(self, base_url):
        Crawler.base_url = base_url

    @staticmethod
    def crawl(thread_name, url, links_to_crawl):
        try:
            print(f"Run with thread {thread_name}")
            link = urljoin(Crawler.base_url, url)
            if (urlparse(link).netloc == 'tutorialedge.net') and (link not in Crawler.crawled_links):
                request = Request(link, headers={'User-Agent': 'Mozilla/5.0'})
                response = urlopen(request, context=Crawler.myssl)
                Crawler.crawled_links.add(link)
                print("Url {} Crawled with Status: {} : {} Crawled In Total".format(response.geturl(), response.getcode(), len(Crawler.crawled_links)))
                soup = BeautifulSoup(response.read(), "html.parser")
                Crawler.enqueueLinks(soup.find_all('a'), links_to_crawl)
        except URLError as e:
            print("URL {} threw this error when trying to parse: {}".format(link, e.reason))
            Crawler.errorLinks.add(link)

    @staticmethod
    def enqueueLinks(links, links_to_crawl):
        for link in links:
            if (urljoin(Crawler.base_url, link.get('href')) not in Crawler.crawled_links):
                if (urljoin(Crawler.base_url, link.get('href')) not in links_to_crawl):
                    links_to_crawl.put(link.get('href'))


class CheckableQueue(queue.Queue):
    def __contains__(self, item):
        with self.mutex:
            return item in self.queue
    def __len__(self):
        return len(self.queue)


THREAD_COUNT = 100
linksToCrawl = CheckableQueue()

def run():
    while True:
        try:
            url = linksToCrawl.get()
            Crawler.crawl(threading.current_thread(), url, linksToCrawl)
        except:
            print("Exception")
        linksToCrawl.task_done()
        return [url]

def append_to_csv(result):
    with open('D:\\omzcloud\\test\\results.csv', 'a') as writer:
        result_writer = csv.writer(writer, delimiter=' ', quotechar='!', quoting=csv.QUOTE_MINIMAL)
        result_writer.writerow(result)


def main():
    url = input("Website > ")
    linksToCrawl.put(url)
    while not linksToCrawl.empty():
        with ThreadPoolExecutor(max_workers=THREAD_COUNT) as executor:
            future = executor.submit(run)
            try:
                if future.result() is not None:
                    append_to_csv(future.result())
            except:
                print(future.exception())

print("Total Links Crawled: {}".format(len(Crawler.crawled_links)))
print("Total Errors: {}".format(len(Crawler.errorLinks)))

if __name__ == '__main__':
    main()
```


---
# Cáº£i tiáº¿n láº§n 2

Trong code trÃªn, do chá»‰ gá»i `.submit()` Ä‘Ãºng 1 láº§n â†” task Ä‘Ã³ chá»‰ Ä‘c assign cho 1 thread tá»« Ä‘áº§u tá»›i cuá»‘i
â‡’ KhÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c tá»‘i Ä‘a sá»©c máº¡nh cá»§a cÃ¡c thread khÃ¡c trong pool

â¡ï¸ Cáº£i tiáº¿n, do dÃ¹ng queue Ä‘Ã£ Ä‘áº£m báº£o khÃ´ng bá»‹ race-condition, do Ä‘Ã³ cÃ³ thá»ƒ push nhiá»u task vÃ o trong pool.

```python
THREAD_COUNT = 100
linksToCrawl = CheckableQueue()

def append_to_csv(result):
    with open('D:\\omzcloud\\test\\results.csv', 'a') as writer:
        result_writer = csv.writer(writer, delimiter=' ', quotechar='!', quoting=csv.QUOTE_MINIMAL)
        result_writer.writerow(result)

def run():
    while True:
        try:
            url = linksToCrawl.get(timeout=3)
            if url is None:  # Sentinel Ä‘á»ƒ káº¿t thÃºc
	            linksToCrawl.task_done()
	            break
            Crawler.crawl(threading.current_thread(), url, linksToCrawl)
            append_to_csv([url])
        except queue.Empty:
            break
        except:
            print("Exception")
        finally:
            linksToCrawl.task_done()

        
def main():
    # url = input("Website > ")
    url = "https://tutorialedge.net"
    linksToCrawl.put(url)
    futures = []
    with ThreadPoolExecutor(max_workers=THREAD_COUNT) as executor:
        for i in range(THREAD_COUNT):
            future = executor.submit(run)
            futures.append(future)
        linksToCrawl.join()
	    # Äá»ƒ cháº¯c cháº¯n Gá»­i tÃ­n hiá»‡u káº¿t thÃºc cho tá»«ng thread
        for _ in range(THREAD_COUNT):
            linksToCrawl.put(None)  # Sentinel
```

***Chi tiáº¿t cÃ¡c cáº£i tiáº¿n***
* **Blocking Queue.get()**: PhÆ°Æ¡ng thá»©cÂ `linksToCrawl.get()`Â máº·c Ä‘á»‹nhÂ **blocking**, tá»©c lÃ  nÃ³ sáº½ chá» Ä‘áº¿n khi cÃ³ pháº§n tá»­ trong hÃ ng Ä‘á»£i. Khi hÃ ng Ä‘á»£i trá»‘ng, cÃ¡c luá»“ng bá»‹ treo táº¡i Ä‘Ã¢y vÃ  khÃ´ng bao giá» thoÃ¡t, dáº«n Ä‘áº¿n chÆ°Æ¡ng trÃ¬nh bá»‹ Ä‘á»©ng
* **Xá»­ lÃ½ ngoáº¡i lá»‡ queue.Empty khÃ´ng hiá»‡u quáº£**: Khá»‘iÂ `except queue.Empty`Â khÃ´ng bao giá» Ä‘Æ°á»£c kÃ­ch hoáº¡t vÃ¬Â `get()`Â khÃ´ng nÃ©m ra ngoáº¡i lá»‡ nÃ y trá»« khi Ä‘Æ°á»£c gá»i vá»›iÂ `block=False`Â hoáº·c cÃ³ ***timeout***.
### CÃ¡ch kháº¯c phá»¥c triá»‡t Ä‘á»ƒ:
1. **Sá»­ dá»¥ng get() vá»›i timeout**:
```python
try:
    url = linksToCrawl.get(timeout=1)  # Chá» 1 giÃ¢y trÆ°á»›c khi kiá»ƒm tra láº¡i
except queue.Empty:
    if linksToCrawl.qsize() == 0:
        break
```
1. **Káº¿t há»£p Queue.join() vÃ  sentinel values**:
    - ThÃªm giÃ¡ trá»‹ Ä‘áº·c biá»‡t (vÃ­ dá»¥:Â `None`) vÃ o hÃ ng Ä‘á»£i Ä‘á»ƒ bÃ¡o hiá»‡u luá»“ng thoÃ¡t.
    - Gá»iÂ `linksToCrawl.join()`Â trongÂ `main()`Â Ä‘á»ƒ Ä‘á»£i má»i task hoÃ n thÃ nh.